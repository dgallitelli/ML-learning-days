{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 1.5/1.6 Discovery\n",
    "\n",
    "This is the very first demo run with PyTorch. The goal of this notebook is to discover the very basics of PyTorch, things like the different cells available (`torch.nn` package) and creating some easy NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the first Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the EasyNet: two layer of Fully Connected NN - in PyTorch terms, `Linear` cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_def.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EasyNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EasyNet, self).__init__()\n",
    "        # A very simple NN, with two \"Dense\" layers\n",
    "        # This will receive 13 values in input (Boston Housing dataset will be used for test)\n",
    "        # Will scale it down to 6, then output only 1 value - this is a Linear Regression problem\n",
    "        self.hidden_1 = nn.Linear(13, 6)\n",
    "        self.hidden_2 = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # How the layers should do the forward pass? \"Sequential\"\n",
    "        # The `tanh` and `sigmoid` are the activation layers for the cells.\n",
    "        x = torch.tanh(self.hidden_1(x))\n",
    "        x = torch.sigmoid(self.hidden_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: does it generate a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_def import EasyNet\n",
    "import torch\n",
    "\n",
    "# Launch the DumbNet\n",
    "model = EasyNet()\n",
    "# Create an input\n",
    "i = torch.randn(13)\n",
    "# Generate the output\n",
    "out = model(i)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the training process. A training process is composed of:\n",
    "\n",
    "- a forward pass - just call the `model` function\n",
    "- an estimation of the error - use an error function such as `RMSE`\n",
    "- a backward propagation of the computed gradients - PyTorch provides a `backward()` function which makes it easy\n",
    "- update the weights of the NN via an optimizer - PyTorch provides `torch.optim`\n",
    " - SGD could also be manual via `weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definition. In this case, MSE (example)\n",
    "target = torch.randn(1) # random target value\n",
    "print('Target: '+str(target))\n",
    "criterion = torch.nn.MSELoss() # Use MSE as Loss\n",
    "loss = criterion(out, target) # Compute the MSE between output and target\n",
    "\n",
    "print('Loss: '+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation of the gradients\n",
    "model.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "loss.backward()     # backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: an example of manual update of the weights via SGD:\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights via an optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop will be:\n",
    "\n",
    "```python\n",
    "# In the training loop:\n",
    "optimizer.zero_grad()               # zero the gradient buffers\n",
    "output = model(i)                   # forward pass\n",
    "loss = criterion(output, target)    # computation of the loss\n",
    "loss.backward()                     # Backpropagation of the loss\n",
    "optimizer.step()                    # Weights update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "The training script should:\n",
    "\n",
    "1. Load a batch of data - via `torch.utils.data.DataLoader`\n",
    "2. Predict the batch of the data - via `model`\n",
    "3. Calculate the loss value by predict value and true value - via `torch.nn.MSELoss()`\n",
    "4. Clear the grad value optimizer stored - via `optimizer.zero_grad()`\n",
    "5. Backpropagate the loss value - via `loss.backward()`\n",
    "6. Update optimizer - via `optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try loading data from the SKLearn Boston Housind dataset\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "x, y = data['data'], data['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them for later use\n",
    "import os, numpy as np\n",
    "local_dir = './'\n",
    "\n",
    "np.save(os.path.join(local_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(local_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(local_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(local_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float).view(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# let’s construct a Dataset of Tensor.\n",
    "datasets = TensorDataset(x_train, y_train)\n",
    "# Then, generate a DataLoder by using this Dataset\n",
    "train_iter = DataLoader(datasets, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_iter:\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch {} loss: {:.4f}\".format(epoch + 1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s check its performance on the testing dataset\n",
    "print(criterion(model(x_test), y_test).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we successfully run a PyTorch training, let's write a Training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse, torch, numpy as np, os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD\n",
    "from model_def import EasyNet\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    # dataset info - we will use numpy\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    # model directory: useful later for SageMaker\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    # return the parsed arguments\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    return x_test, y_test\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters and hyper-parameters\n",
    "    args, _ = parse_args()\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "    \n",
    "    # Load the dataset\n",
    "    x_train, y_train = get_train_data(args.train)\n",
    "    x_test, y_test = get_test_data(args.test)\n",
    "    # Parse it to torch.tensor\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float).view(-1, 1)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float).view(-1, 1)\n",
    "    # Create the TensorDataset and the DataLoader\n",
    "    datasets = TensorDataset(x_train, y_train)\n",
    "    train_iter = DataLoader(datasets, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup NN, Loss, and SGD\n",
    "    model = EasyNet()\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_iter:\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch {} loss: {:.4f}\".format(epoch + 1, loss.item()))\n",
    "        \n",
    "    # Evaluate on test dataset\n",
    "    print('MSE: '+str(criterion(model(x_test), y_test).item()))\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(args.model_dir, 'model.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('Model stored at: '+model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the new training script (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "    --epochs 5 \\\n",
    "    --batch_size 128 \\\n",
    "    --learning_rate 0.01 \\\n",
    "    --train . \\\n",
    "    --test . \\\n",
    "    --model_dir ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
