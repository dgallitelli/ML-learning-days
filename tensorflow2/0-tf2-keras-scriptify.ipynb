{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR**: do you just want to obtain the model and the training script? Run the two cells with `%%writefile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 and Keras Discovery\n",
    "\n",
    "This is the very first demo run with TensorFlow. The goal of this notebook is to discover the very basics of TensorFlow, things like the different cells available in Keras and creating some easy NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the first Neural Network in TensorFlow 2 and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_def.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(13, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x1 = self.dense1(inputs)\n",
    "        x2 = self.dense2(x1)\n",
    "        y = self.out(x2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: does it generate a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_def import MyModel\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# Launch the DumbNet\n",
    "model = MyModel()\n",
    "# Create an input\n",
    "i = Input(shape=(13,))\n",
    "# Generate the output\n",
    "out = model(i)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "# Loss definition. In this case, MSE (example)\n",
    "target = Input(shape=(1,)) # random target value\n",
    "print('Target: '+str(target))\n",
    "loss = MSE(target, out) # Use MSE as Loss\n",
    "\n",
    "print('Loss: '+str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras abstracts away the complexity of the training loop with the `fit` call. We will have:\n",
    "\n",
    "```python\n",
    "# Define the model and the optimizer\n",
    "model = MyModel()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "# Compile the model with the optimizer and the chosen loss\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "# evaluate on test set\n",
    "scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "The training script should:\n",
    "\n",
    "1. Load the data via `np.load()` from our input\n",
    "2. Set the hyperparameters from our inputs (epochs, batch_size, etc)\n",
    "3. Choose the correct device\n",
    "4. Load the model, the optimizer and compile it for a specific loss\n",
    "5. Start the training with `fit()`\n",
    "6. Evaluate results with `evaluate()`\n",
    "7. Save the model with `save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from model_def import MyModel\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "    return x_train, y_train\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "    return x_test, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 0. Load the arguments\n",
    "    args, _ = parse_args()\n",
    "    # 1. Load the dataset\n",
    "    print('Training data location: {}'.format(args.train))\n",
    "    print('Test data location: {}'.format(args.test))\n",
    "    x_train, y_train = get_train_data(args.train)\n",
    "    x_test, y_test = get_test_data(args.test)\n",
    "    # 2. Load the hyperparameters\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "    # 3. Choose the device \n",
    "    device = '/cpu:0' \n",
    "    print(device)\n",
    "    with tf.device(device):\n",
    "        # 4. Load model, optimizer and compile\n",
    "        model = MyModel()\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')    \n",
    "        # 5. Train the model on the training dataset\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                  validation_data=(x_test, y_test))\n",
    "        # 6. Evaluate on test set\n",
    "        scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "        print(\"\\nTest MSE :\", scores)\n",
    "        # 7. Save the model\n",
    "        model.save(args.model_dir + '/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try loading data from the SKLearn Boston Housind dataset\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "x, y = data['data'], data['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them for later use\n",
    "import os, numpy as np\n",
    "local_dir = './'\n",
    "\n",
    "np.save(os.path.join(local_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(local_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(local_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(local_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the new training script (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "    --epochs 5 \\\n",
    "    --batch_size 128 \\\n",
    "    --learning_rate 0.01 \\\n",
    "    --train . \\\n",
    "    --test . \\\n",
    "    --model_dir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
